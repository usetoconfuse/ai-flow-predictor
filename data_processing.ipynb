{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data processing functions\n",
    "\n"
   ],
   "id": "821573af85a9827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "3439432d73a6480c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialisation",
   "id": "8e1913a44b2479db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:35:36.739196Z",
     "start_time": "2025-03-23T06:35:36.728996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read/write csv files\n",
    "def read_raw_csv():\n",
    "    path = \"datasets/raw/raw.csv\"\n",
    "    print(f\"Loading raw data from {path}\")\n",
    "    return pd.read_csv(path, header=[0, 1], index_col=0)\n",
    "\n",
    "\n",
    "def read_processed_csv(filename):\n",
    "    path = f\"datasets/processed/{filename}.csv\"\n",
    "    print(f\"Loading processed data from {path}\")\n",
    "    return pd.read_csv(path, header=[0,1], index_col=[0,1])\n",
    "\n",
    "\n",
    "def write_processed_csv(dataset, filename):\n",
    "    path = f\"datasets/processed/{filename}.csv\"\n",
    "    dataset.to_csv(path)\n",
    "    print(f\"Dataset saved to {path}\")\n",
    "\n",
    "\n",
    "# Read and anonymise Excel data into DataFrame\n",
    "# Should only be used once on an Excel sheet, after that\n",
    "# use read_raw_csv as this saves anonymising data again\n",
    "def read_river_excel(file):\n",
    "    # Get data from file and format frame for program\n",
    "    df = pd.read_excel(file, header=[0, 1], index_col=0)\n",
    "    df.columns = df.columns.set_levels([\"r\", \"f\", \"p\"], level=0)\n",
    "    df.index = df.index.to_series().dt.date\n",
    "    df = df.rename_axis(index=\"date\")\n",
    "    df = df.rename_axis(columns=[\"type\", \"src\"])\n",
    "\n",
    "    # Anonymise data\n",
    "    # Iterate over columns and rename by type and number\n",
    "    # Rightmost flow column is chosen as the predictor p\n",
    "    name_dict = {}\n",
    "    i = 1\n",
    "    for col in df[\"f\"]:\n",
    "        name_dict.update({col: f\"f{i}\"})\n",
    "        i += 1\n",
    "    name_dict.update({df.columns[i-2][1]: \"p\"})\n",
    "    i = 1\n",
    "    for col in df[\"r\"]:\n",
    "        name_dict.update({col: f\"r{i}\"})\n",
    "        i += 1\n",
    "    df = df.rename(columns=name_dict)\n",
    "\n",
    "    # Move predictor column to the end\n",
    "    df = df[[c for c in df if c[1] != \"p\"]\n",
    "            + [c for c in df if c[1] == \"p\"]]\n",
    "    return df"
   ],
   "id": "d040bc8df6d99647",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning",
   "id": "50c59b0a16142f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Discard spurious data (set to NaN)\n",
    "def remove_spurious_data(df):\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df = df[df >= 0]\n",
    "    df[\"r\"] = df[\"r\"][df[\"r\"] <= 279]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Std dev culling\n",
    "def cull_by_sd(df, val):\n",
    "    return df[np.abs(df - df.std()) <= df.mean() + val * df.std()]"
   ],
   "id": "94c471773b8b43b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Exploration",
   "id": "a7459d3a7142690a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lag a column by x days\n",
    "def lag_column(frame, col, days):\n",
    "    if days < 0: return\n",
    "    frame[col] = frame[col].shift(days)\n",
    "    frame = frame.rename(columns={col[1]: col[1] + f\" (t-{days})\"})\n",
    "    return frame"
   ],
   "id": "d6e5d23b2f643784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting",
   "id": "454b08d42d808d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into train, validation, test\n",
    "# Return a dictionary of the three DataFrames\n",
    "def split_data(df, trn_frac, val_frac, test_frac):\n",
    "\n",
    "    # Adjust the fractions to account for values removed as they were sampled\n",
    "    v_f = val_frac / (1-trn_frac)\n",
    "    ts_f = test_frac / (1-trn_frac-val_frac)\n",
    "\n",
    "    trn_df = df.sample(frac=trn_frac)\n",
    "    df = df.drop(index=trn_df.index)\n",
    "\n",
    "    val_df = df.sample(frac=v_f)\n",
    "    df = df.drop(index=val_df.index)\n",
    "\n",
    "    test_df = df.sample(frac=ts_f)\n",
    "\n",
    "    # Recombine datasets and add to index which dataset each row is in\n",
    "    data = pd.concat([trn_df, val_df, test_df], keys=[\"trn\",\"val\",\"test\"], names=[\"dataset\",])\n",
    "\n",
    "    return data"
   ],
   "id": "9e325b8e1a6a0043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standardisation",
   "id": "6abbdffce4a1b6d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standardise a split dataset's columns within a range\n",
    "def standardise_data(data, min_range, max_range):\n",
    "    for col in data:\n",
    "        min_val = min(data.loc[\"trn\"][col].min(),\n",
    "                      data.loc[\"val\"][col].min())\n",
    "        max_val = max(data.loc[\"trn\"][col].max(),\n",
    "                      data.loc[\"val\"][col].max())\n",
    "        data[col] = data[col].apply(lambda x:\n",
    "            (max_range-min_range) * ((x-min_val) / (max_val-min_val) + min_range))\n",
    "    return data"
   ],
   "id": "c1ec53dc32aec141",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
